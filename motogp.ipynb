import pandas as pd
import numpy as np
import lightgbm as lgb
from lightgbm import LGBMRegressor, early_stopping, log_evaluation
from sklearn.preprocessing import OrdinalEncoder
from sklearn.metrics import mean_squared_error, r2_score
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
warnings.filterwarnings("ignore")

df1 = pd.read_csv("train.csv", encoding='latin1', engine='python', on_bad_lines='skip')
df2 = pd.read_csv("val.csv", encoding='latin1', engine='python', on_bad_lines='skip')
df3 = pd.read_csv("test.csv", encoding='latin1', engine='python', on_bad_lines='skip')
submit = pd.read_csv("sample_submission.csv")

y_col = 'Lap_Time_Seconds'

df1[y_col] = pd.to_numeric(df1[y_col], errors='coerce')
df2[y_col] = pd.to_numeric(df2[y_col], errors='coerce')
df1.dropna(subset=[y_col], inplace=True)
df2.dropna(subset=[y_col], inplace=True)

y_train = df1[y_col]
y_val = df2[y_col]

ftrs = [f for f in df1.columns if f != y_col and f in df2.columns and f in df3.columns]

X_train = df1[ftrs].copy()
X_val = df2[ftrs].copy()
X_test = df3[ftrs].copy()

cat_cols = X_train.select_dtypes(include='object').columns.tolist()
num_cols = X_train.select_dtypes(include=np.number).columns.tolist()

for col in num_cols:
    med = X_train[col].median()
    X_train[col].fillna(med, inplace=True)
    X_val[col].fillna(med, inplace=True)
    X_test[col].fillna(med, inplace=True)

for col in cat_cols:
    for df in [X_train, X_val, X_test]:
        df[col].fillna('Unknown', inplace=True)

if 'Rider' in X_train.columns and 'Bike' in X_train.columns:
    for df in [X_train, X_val, X_test]:
        df['Rider_Bike'] = df['Rider'] + '_' + df['Bike']
    cat_cols.append('Rider_Bike')

if 'Rider' in X_train.columns:
    avg_lap = df1.groupby('Rider')[y_col].mean().to_dict()
    for df in [X_train, X_val, X_test]:
        df['Rider_Avg_Lap'] = df['Rider'].map(avg_lap)
    num_cols.append('Rider_Avg_Lap')

enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
X_train[cat_cols] = enc.fit_transform(X_train[cat_cols].astype(str))
X_val[cat_cols] = enc.transform(X_val[cat_cols].astype(str))
X_test[cat_cols] = enc.transform(X_test[cat_cols].astype(str))

reg = LGBMRegressor(
    objective='regression',
    learning_rate=0.05,
    estimators=1000,
    num_leaves=64,
    feature_fraction=0.85,
    bagging_fraction=0.85,
    bagging_freq=5,
    random_state=42
)

reg.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    eval_metric='rmse',
    callbacks=[early_stopping(50), log_evaluation(100)],
    categorical_feature=cat_cols
)

pred_val = reg.predict(X_val)
rmse = np.sqrt(mean_squared_error(y_val, pred_val))
r2 = r2_score(y_val, pred_val)

print(f"‚úÖ RMSE on Validation Set: {rmse:.4f}")
print(f"‚úÖ R¬≤ Score on Validation Set: {r2:.4f}")

preds_test = reg.predict(X_test)
submit[y_col] = preds_test
submit.to_csv("submission.csv", index=False)
print("üìÅ submission.csv saved!")

resid = y_val - pred_val
plt.figure(figsize=(10, 6))
sns.histplot(resid, bins=50, kde=True, color='royalblue')
plt.title('Residuals Distribution')
plt.xlabel('Residual')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_val, y=pred_val, alpha=0.6)
plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted')
plt.grid(True)
plt.show()

plt.figure(figsize=(12, 8))
sns.barplot(x=reg.feature_importances_, y=X_train.columns, palette="viridis")
plt.title('Feature Importances')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()
